<!DOCTYPE html>
<html>
    <head>
        <meta charSet="UTF-8" />
        <title>SeungHeon Doh | MIR, ML/DL Researcher</title>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.css">
        <script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/addon/runmode/runmode-standalone.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/mode/python/python.min.js"></script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                tex2jax: { inlineMath: [['$$', '$$']], displayMath: [['$$$', '$$$']] }
            });
        </script>
        <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŽ¹</text></svg>"></link>
        <meta httpEquiv="x-ua-compatible" content="ie=edge" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="description" content="" />
        <meta name="keywords" content="" />
        <meta name="robots" content="index, follow, noodp" />
        <meta name="googlebot" content="index, follow" />
        <meta name="google" content="notranslate" />
        <meta name="format-detection" content="telephone=no" />
        <title>SeungHeonDoh</title>
    </head>
    <style>
        header{        
            padding-top: 1.91rem;
        }
        header a{
            color: gray !important;
            text-decoration: none;
        }
        h1, h2, h3, h4, h5, h6 {
            font-family: "helvetica";
            margin-bottom: 1.01rem;
        }

        p{
            margin-bottom: 10px;
            line-height: 1.5;
        }

        h1 {
            font-size: 5rem;
            line-height: 1.15;
            }

        h2 {
            font-size: 4rem;
            line-height: 1.11;
        }

        h3 {
            font-size: 2rem;
            line-height: 1.74;
        }

        h4 {
            font-size: 1.4rem;
            line-height: 1.39;
        }

        h5 {
            font-size: 1.2rem;
            line-height: 1.56;
            margin-bottom: 0.5em;
        }

        h1, h2, h3, h4, h5, h6 {
            line-height: 1.56;
            margin-bottom: 1.01rem;
        }

        .main table {
            display: inline-table;
        }
        table {
            table-layout:fixed;
            width: 100%;
            overflow: hidden;
        }
        #player{
            width: 100%;
        }
        img, svg {
            max-width: 100%;
            height: auto;
        }

        .blog_contents{
            margin-bottom: 1rem;
        }

        .footer{
            height: 50px;
            background-color: white;
        }

        .wrapper {
            max-width: 1920px;
            margin: auto;
            padding-left: 20rem;
            padding-right: 20rem;
            }
        @media(max-width: 1700px){
            .wrapper {
                padding-left: 8rem;
                padding-right: 8rem;
            }
            }

        @media(max-width: 1199px){
            .wrapper {
            padding-left: 5rem;
            padding-right: 5rem;
            }
        }

        @media(max-width: 575px){
            .wrapper {
            padding-left: 1.25rem;
            padding-right: 1.25rem;
            }
        }
    </style>
    
    <body style="background-color: #f8f8f8;">
        <header id="header" class="site-header">
            <div class="wrapper">
                <a 
                    title="nav"
                    class="btn btn-link transform-scale-h border-0 p-0"
                    href="https://seungheondoh.github.io/#/">Home</a>
            </div>
        </header>
    
        <main id="main" class="site-main">
            <div class="wrapper">
                <h3>Textless Speech-to-Music Retrieval Using Emotion Similarity</h3>
                    <pre><code>Textless Speech-to-Music Retrieval Using Emotion Similarity, ICASSP 2023 (submitted) - SeungHeon Doh, Minz Won, Keunwoo Choi, Juhan Nam
                    </code></pre>

            <p>This project maps <strong>speech</strong> and <strong>music</strong> to the joint embedding space and supports music item search for speech query by calculating the similarity between them.
            The detail of the methodology for building the dataset please refer to our paper.</p>
            <ul>
                <li>Paper on Arxiv (will be updated)</li>
                <li><a href="https://github.com/SeungHeonDoh/speech-to-music">Implementation Code</a></li>
                <li><a href="https://zenodo.org/record/7341484">Pre-trained model on Zenodo</a></li>
            </ul>
            <img class="blog_contents" src="assets/img/main.png" alt="speech_to_music"/>
            <h4>Abstract</h4>
                <p>We introduce a framework that recommends music based on the emotions of speech. In content creation and daily life, speech contains information about human emotions, which can be enhanced by music. In our framework, we focus on a cross-domain retrieval to bridge the gap between speech and music via emotion labels. We explore different speech representations and report their impact on different speech types including acted speech, lexically-matched speech, and wake-up word speech. We also propose an emotion similarity regularization term in cross-domain retrieval tasks. By incorporating the regularization term into training, similar speech-and-music pairs in the emotion space are closer in the joint embedding space. Our comprehensive experimental results show that the propose model is effective in textless speech-to-music retrieval. </p>
            <img class="blog_contents" src="assets/img/emosim.png" alt="speech_to_music"/>
            <h4>Emotion Similarity Regularization</h4>
                <p> To preserve the continuous emotion distribution in joint embedding embedding space, we propose emotion similarity regularization (EmoSim), modified version of RankSim, for cross-domain retrieval task. 
                    An overview of our approach is shown in figure. In practice, our goal is to encourage alignment between the similarity of neighbors in emotion space and the similarity of neighbors in feature space. 
                </p>
                <pre><code id="python_code">
                    import torch
                    import torch.nn.functional as F

                    # code reference: https://github.com/BorealisAI/ranksim-imbalanced-regression
                    def batchwise_emotion_regularizer(S_z, S_y):
                        """
                        S_z: feature similarity between speech and music
                        S_y: emotion similarity between speech and music
                        """
                        unique_batch = torch.unique(S_y, dim=1)
                        unique_pred = []
                        for pred, target, unique_item in zip(S_z, S_y, unique_batch):
                            indices = torch.stack([random.choice((target==i).nonzero()[0]) for i in unique_item])
                            unique_pred.append(pred[indices])
                        emotion_feature = torch.stack(unique_pred)
                        emo_loss = F.mse_loss(emotion_feature, unique_batch)
                        return emo_loss
                </code></pre>
                <script type="text/javascript">
                    window.onload = function(){
                        var codeElement = document.getElementById('python_code');
                        // Add code mirror class for coloring (default is the theme)
                        codeElement.classList.add( 'cm-s-default' );
                        var code = codeElement.innerText;
            
                        codeElement.innerHTML = "";
            
                        CodeMirror.runMode(
                          code,
                          'python',
                          codeElement
                        );
                    };
                </script>
            <h4>In the wild demo</h4>
                <p>
                To demonstrate real industrial scenarios, we use samples from audio books contents (youtube). 
                It also searches for music using only the audio modality because it assumes no transcription. It also makes use of the <a href='https://www.jyu.fi/hytk/fi/laitokset/mutku/en/research/projects2/past-projects/coe/materials/emotion/soundtracks/Index'>soundtrack360 dataset</a> as a high-quality audio database.
                </p>
            <iframe class="blog_contents" width="800" height="600" src="https://www.youtube.com/embed/kr0DqnK3vso" title="YouTube video player" frameborder="0" allowfullscreen></iframe>
            <h4>Test-set demo</h4>
                <p> We report the results for IEMOCAP and Audioset, which are test datsets reported in the paper.
                <table>
                    <tr>
                        <th>  </th>
                        <th> Speech Query </th>
                        <th> Similar Music 1 </th>
                        <th> Similar Music 2 </th>
                        <th> Similar Music 3 </th>
                    </tr>
                    <tr> 
                        <th> angry </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/angry/angry.wav" type="audio/mpeg"></audio> </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/angry/angry (1).wav" type="audio/mpeg"></audio> </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/angry/angry (2).wav" type="audio/mpeg"></audio> </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/angry/angry (3).wav" type="audio/mpeg"></audio> </th>
                    </tr>
                    <tr> 
                        <th> happy </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/happy/happy.wav" type="audio/mpeg"></audio> </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/happy/happy (1).wav" type="audio/mpeg"></audio> </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/happy/happy (2).wav" type="audio/mpeg"></audio> </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/happy/happy (3).wav" type="audio/mpeg"></audio> </th>
                    </tr>
                    <tr> 
                        <th> sad </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/sad/sad.wav" type="audio/mpeg"></audio> </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/sad/sad (1).wav" type="audio/mpeg"></audio> </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/sad/sad (2).wav" type="audio/mpeg"></audio> </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/sad/sad (3).wav" type="audio/mpeg"></audio> </th>
                    </tr>
                    <tr> 
                        <th> neutral </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/neutral/neutral.wav" type="audio/mpeg"></audio> </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/neutral/neutral (1).wav" type="audio/mpeg"></audio> </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/neutral/neutral (2).wav" type="audio/mpeg"></audio> </th>
                        <th> <audio controls id="player" onplay="pauseOthers(this);"><source src="assets/audios/neutral/neutral (3).wav" type="audio/mpeg"></audio> </th>
                    </tr>
                </table>

            <h4>Results</h4>
            <img class="blog_contents" src="assets/img/results.png"/> 
            <p>
                Table compares the different retrieval models with text, audio, fusion speech modality. Each cross-modal retrieval model was trained 5 times with a different random seed, and average score and standard deviation are reported together. In the case of the fusion method, we utilize inter-modality dynamics through late fusion. 
                When comparing modalities, audio shows high performance regardless of the speech dataset. In IEMOCAP with linguistic information, text modality outperforms speech modality. A possible reason might be the availability of high emotional clues in the linguistic structure. The fusion model shows a clear performance improvement over other uni-modalities.
                 It is expected that acoustic information and linguistic information have different information.
            </p>
            <p>
                When comparing each method, our proposed emotion similarity (EmoSim) regularization method shows high performance and low standard deviation in all three datasets and various modalities. In particular, in the audio modality of the three datasets, Triplet + EmoSim method shows high NDCG@5 performance. This is because, in the failure case, the model retrieves music items as close as possible to the correct answers.
            </p>

            <h4>Visualization</h4>
            <img class="blog_contents" src="assets/img/viz.png" width="50%" style="float: right;"/> 
            <p>
                Metric learning embedding spaces are projected to a 2D space using uniform manifold approximation and projection (UMAP). We fit UMAP with music and speech embeddings, then projected them into the 2D space. 
                Figure is the visualization of the multi-modal embedding space. Due to space constraints, we only visualize the four models. All embedding vectors are computed from the test set and the star marker stands for speech embedding and the circle marker stands for music embedding.
                First, we confirmed that all models successfully discriminate emotion semantics. In the case of the triplet model, scary music embeddings are located at between sad and happy speech embedding cluster.  This problem is alleviated in Triplet + EmoSim model. 
                There are relatively few scary music samples closer to angry and frustration clusters. We believe that joint embedding space learned inter-modality neighborhood structure from the continuous emotion similarity.

            </p>
            <h4>Conclusion</h4>
            <p>
                We presented a speech-to-music cross-domain retrieval framework that finds music that matches the emotion of speech. We explored various speech modality representations and proposed efficient masking fusion methods. To evaluate joint embedding space, we proposed a new evaluation metric that considers the rank order of a continuous distribution using the valence-arousal space of emotion. Experiments show that our model successfully bridges the modality gap. Especially, our proposed masking fusion methods make a more generalized joint embedding space. In the future, our approach can be extended to include other modalities (e.g., vision) and improved with better hierarchical fusion methods. 
            </p>
        </main>
        <div class="footer"></div>
            </div>
    </body>
</html>
